\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english,russian]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Сравнительный Анализ моделей нейронных сетей лингвистического направления}
\author{Выполнил студент гр.22-ВТм Никитенков Владислав}

\begin{document}
\maketitle

\begin{abstract}
Чему посвящена статья*

Ключевые слова:*
\end{abstract}

\section{Введение}
План:

1)Что такое нейронные сети

2)Классификация

3)Какие из них принадлежат к лингвистическому направлению.

4)Описание каждой системы и ее роль в лингвистике

Что такое нейронные сети:
Нейронные сети – это математические модели, которые состоят из соединенных узлов (нейронов), работающих параллельно, чтобы обрабатывать данные. Они вдохновлены работой человеческого мозга и способны извлекать сложные закономерности из больших объемов информации.

Классификация:
Нейронные сети могут быть классифицированы по различным критериям, таким как архитектура (например, сверточные, рекуррентные), тип обучения (надзорное, ненадзорное, усиленное) и так далее.

Лингвистическое направление:
Среди разнообразных видов нейронных сетей, некоторые из них имеют сильное применение в лингвистике. К примеру, сверточные нейронные сети (CNN) и рекуррентные нейронные сети (RNN), включая вариации типа LSTM (долгая краткосрочная память) и GRU (взвешенные ворота обновления), широко применяются для обработки текста и решения задач в области естественного языка.

Описание каждой системы и их роль в лингвистике:
Это включает в себя описание архитектуры каждой системы, их возможности в обработке текста, а также специфические решаемые задачи в лингвистике, например, машинный перевод, анализ тональности, генерация текста и другие.
\section{Актуальность}
\section{Новизна}
\section{Обзор литературы}
В работе \cite{greenwade93} сделано все плохо
\section{Описание работы}
*Описание работы*
\section{Научная и практическая ценность}
Практическая:\newlineНаучная:
\section{Заключение}


\bibliographystyle{alpha}
\bibliography{sample}
\begin{thebibliography} {40}
\bibitem{1}
Бурнашев Р.Ф., Аламова А.С. Квантитативная лингвистика и искусственный интеллект //Science and Education. - 2022. - Т. 3. - №. 11. - С. 1390- 1402.;
\bibitem{2}
 Бурнашев Р. Ф., Фаррухова Ф. Ш. Лингвистический корпус как база для организации информационного поиска //Science and Education. - 2021. - Т. 2. - №. 3;

\bibitem{3} 
Мансур Ж. Д. Н. З., Саттарова А. Т., Бурнашев Р. Ф. Роль лингвистических корпусов в создании и совершенствовании систем машинного перевода //Science and Education. - 2022. - Т. 3. - №. 2. - С. 1348-1358.;

\bibitem{4} 
Мардиева Р. А. и др. Обучение иностранным языкам с помощью IT технологий //Science and Education. - 2022. - Т. 3. - №. 6. - С. 1173-1180.;
\bibitem{5} 
Kun Jing, Jungang Xu A Survey on Neural Network Language Models [Электронный ресурс]. Режим доступа: https://arxiv.org/abs/1906.03591

\bibitem{6} 
M. Sundermeyer; I. Oparin; J.-L. Gauvain; B. Freiberg; R. Schlüter; H. Ney Comparison of feedforward and recurrent neural network language models [Электронный реусурс]. Режим доступа: https://ieeexplore.ieee.org/abstract/document/6639310/

\bibitem{7} 
 David Luan; Rewon Child Better language models and their implications [Электронный ресурс].
Режим доступа: https://openai.com/research/better-language-models

\bibitem{8} 
Tom B. Brown Language Models are Few-Shot Learners [Электронный реусурс]. Режим доступа: 
https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf

\bibitem{09} 
Gerhard Paaß, Sven Giesselbach Pre-trained Language Models [Электронный ресурс]. Режим доступа: {https://link.springer.com/chapter/10.1007/978-3-031-23190-2\_2}

\bibitem{10} 
Wei Emma Zhang, Quan Z. Sheng, Munazza Zaib A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP [Электронный ресурс]. Режим доступа: https://www.researchgate.net/publication/338931711_A_Short_Survey_of_Pretrained_Language_Models_for_Conversational_AI-A_New_Age_in_NLP

\bibitem{11} 
Ryan Kiros, Ruslan Salakhutdinov, Rich Zemel Multimodal Neural Language Models [Электронный ресурс]. Режим доступа: https://proceedings.mlr.press/v32/kiros14.html

\bibitem{12} 
Aaron Tuor, Ryan Baerwolf, Nicolas Knowles, Brian Hutchinson, Nicole Nichols, Robert Jasper Recurrent Neural Network Language Models for Open Vocabulary Event-Level Cyber Anomaly Detection  [Электронный ресурс]. Режим доступа: https://cdn.aaai.org/ocs/ws/ws0489/17039-75960-1-PB.pdf

\bibitem{13}
Yoon Kim, Yacine Jernite, David Sontag, Alexander RushCharacter-Aware Neural Language Models [Электронный ресурс].
Режим доступа: https://ojs.aaai.org/index.php/AAAI/article/view/10362

\bibitem{14}
Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell Language Models for Image Captioning: The Quirks and What Works [Электронный ресурс]. Режим доступа: https://arxiv.org/abs/1505.01809

\bibitem{15}
Ben Athiwaratkun, Jack W. Stokes Malware classification with LSTM and GRU language models and a character-level CNN [Электронный ресурс]. Режим доступа: https://ieeexplore.ieee.org/abstract/document/7952603

\bibitem{16}
Liyuan Liu, Jingbo Shang, Xiang Ren, Frank Xu, Huan Gui, Jian Peng, Jiawei Han Empower Sequence Labeling with Task-Aware Neural Language Model [Электронный реусурс]. Режим доступа: https://ojs.aaai.org/index.php/AAAI/article/view/12006

\bibitem{17}
Aytug Onan, Mansur Alp Toçoğlu A Term Weighted Neural Language Model and Stacked Bidirectional LSTM Based Framework for Sarcasm Identification [Электронный ресурс]. Режим доступа: https://ieeexplore.ieee.org/abstract/document/9316208

\bibitem{18}
Наталья Викторовна Картечина ВИДЫ НЕЙРОННЫХ СЕТЕЙ И ИХ ПРИМЕНЕНИЕ [Электронный ресурс]. Режим доступа: http://www.opusmgau.ru/index.php/see/article/download/3835/3817

\bibitem{19}
 [Электронный ресурс] Режим доступа: https://ds.amu.edu.et/xmlui/bitstream/handle/123456789/4338/an-introduction-to-neural-networks.9781857286731.36028.pdf?sequence=1&isAllowed=y

\bibitem{20}
Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu, Xuanjing HuangLong Short-Term Memory Neural Networks for Chinese Word Segmentation [Электронный ресурс].
Режим доступа: https://aclanthology.org/D15-1141.pdf

\bibitem{21}
Siwei Lai, Liheng Xu, Kang Liu, Jun Zhao ОRecurrent Convolutional Neural Networks for Text Classification [Электронный ресурс].
Режим доступа: https://ojs.aaai.org/index.php/AAAI/article/view/9513

\bibitem{22}
Yoav Goldberg Neural Network Methods for Natural Language Processing [Электронный ресурс]. Режим доступа: https://books.google.ru/books?hl=ru&lr=&id=64hyEAAAQBAJ&oi=fnd&pg=PP1&dq=neural+network+words&ots=t7MaEQx88B&sig=TWscN7h1VVdyJfZ-AENMLV8hZrk&redir_esc=y#v=onepage&q=neural%20network%20words&f=false

\bibitem{23}
Wim De Mulder, Steven Bethard, Marie-Francine Moens A survey on the application of recurrent neural networks to statistical language modeling [Электронный ресурс].
Режим доступа: https://www.sciencedirect.com/science/article/pii/S088523081400093X

\bibitem{24}
Jianpeng Cheng, Mirella Lapata Neural Summarization by Extracting Sentences and Words [Электронный ресурс]. Режим доступа: https://arxiv.org/abs/1603.07252

\bibitem{25}
Martin Sundermeyer, Hermann Ney, Ralf Schlüter From Feedforward to Recurrent LSTM Neural Networks for Language Modeling [Электронный ресурс].
Режим доступа: https://ieeexplore.ieee.org/abstract/document/7050391

\bibitem{26}
Marcel Trotzek, Sven Koitka, Christoph M. Friedrich Utilizing Neural Networks and Linguistic Metadata for Early Detection of Depression Indications in Text Sequences [Электронный ресурс].
Режим доступа: https://ieeexplore.ieee.org/abstract/document/8580405

\bibitem{27}
Yoav Goldberg A Primer on Neural Network Models for Natural Language Processing [Электронный ресурс].
Режим доступа: https://www.jair.org/index.php/jair/article/view/11030

\bibitem{28}
Mingbo Ma, Liang Huang, Bing Xiang, Bowen Zhou Dependency-based Convolutional Neural Networks for Sentence Embedding [Электронный ресурс]. Режим доступа: https://arxiv.org/abs/1507.01839

\bibitem{29}
Fred Richardson, Douglas Reynolds, Najim Dehak Deep Neural Network Approaches to Speaker and Language Recognition [Электронный ресурс]. Режим доступа: https://ieeexplore.ieee.org/abstract/document/7080838

\bibitem{30}
Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman Reading Text in the Wild with Convolutional Neural Networks [Электронный ресурс]. Режим доступа: https://link.springer.com/article/10.1007/s11263-015-0823-z
\bibitem{31}
Jakub Nowak, Ahmet Taspinar, Rafał Scherer  LSTM Recurrent Neural Networks for Short Text and Sentiment Classification [Электронный ресурс]. Режим доступа: https://link.springer.com/chapter/10.1007/978-3-319-59060-8_50

\bibitem{32}
Yi-Chao Wu, Fei Yin, Cheng-Lin Liu Improving handwritten Chinese text recognition using neural network language models and convolutional neural network shape models [Электронный ресурс]. Режим доступа: https://www.sciencedirect.com/science/article/abs/pii/S0031320316304472

\bibitem{33}
Sercan Ö. Arık, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yongguo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, Shubho Sengupta, Mohammad ShoeybiDeep Voice: Real-time Neural Text-to-Speech [Электронный ресурс].
Режим доступа: https://proceedings.mlr.press/v70/arik17a.html?ref=https://githubhelp.com

\bibitem{34}
Ottokar Tilk, Tanel Alumae Bidirectional Recurrent Neural Network with Attention Mechanism for
Punctuation Restoration [Электронный ресурс]. Режим доступа: https://www.researchgate.net/profile/Ottokar-Tilk/publication/307889284_Bidirectional_Recurrent_Neural_Network_with_Attention_Mechanism_for_Punctuation_Restoration/links/57ed346708ae26b51b395be1/Bidirectional-Recurrent-Neural-Network-with-Attention-Mechanism-for-Punctuation-Restoration.pdf

\bibitem{35}
Shamil Chollampatt, Hwee Tou Ng A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction [Электронный ресурс]. Режим доступа: https://ojs.aaai.org/index.php/AAAI/article/view/12069

\bibitem{36}
Zhong-Liang Yang, Xiao-Qing Guo, Zi-Ming Chen, Yong-Feng Huang, Yu-Jin Zhang RNN-Stega: Linguistic Steganography Based on Recurrent Neural Networks [Электронный ресурс]. Режим доступа: https://ieeexplore.ieee.org/abstract/document/8470163

\bibitem{37}
Dipti Pawade, Avani Sakhapara, Mansi Jain, Neha Jain, Krushi Gada Story Scrambler - Automatic Text Generation Using Word Level RNN-LSTM [Электронный ресурс]. Режим доступа: https://www.mecs-press.org/ijitcs/ijitcs-v10-n6/IJITCS-V10-N6-5.pdf

\bibitem{38}
Xu-Yao Zhang, Fei Yin, Yan-Ming Zhang, Cheng-Lin Liu, Yoshua Bengio Drawing and Recognizing Chinese Characters with Recurrent Neural Network [Электронный ресурс]. Режим доступа: hhttps://ieeexplore.ieee.org/abstract/document/7903730

\bibitem{39}
Zhizheng Wu, Oliver Watts, Simon King Merlin: An Open Source Neural Network Speech Synthesis System [Электронный ресурс]. Режим доступа: hhttp://ssw9.talp.cat/papers/ssw9_PS2-13_Wu.pdf

\bibitem{40}
Большая языковая модель [Электронный ресурс]. Режим доступа: https://ru.wikipedia.org/wiki/%D0%91%D0%BE%D0%BB%D1%8C%D1%88%D0%B0%D1%8F_%D1%8F%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C

\end{thebibliography}

\end{document}
